{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyvzNxjdpuIm"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "collected_jobs = set()  # ì¤‘ë³µ ë°©ì§€ìš© set\n",
    "data_list = []\n",
    "total_pages = 10  # ì›í•˜ëŠ” í˜ì´ì§€ ê°œìˆ˜\n",
    "\n",
    "base_url = \"https://www.saramin.co.kr/zf_user/search/recruit\"\n",
    "\n",
    "for page in tqdm(range(1, total_pages + 1), desc=\"í¬ë¡¤ë§ ì§„í–‰ì¤‘\", unit=\"í˜ì´ì§€\"):\n",
    "    params = {\n",
    "        \"searchType\": \"search\",\n",
    "        \"company_cd\": \"0,1,2,3,4,5,6,7,9,10\",\n",
    "        \"searchword\": \"ë°ì´í„°ë¶„ì„ê°€\",\n",
    "        \"recruitPage\": page,\n",
    "        \"recruitSort\": \"relation\",\n",
    "        \"recruitPageCount\": 40,\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    for tag in soup.select(\"div.item_recruit\"):\n",
    "        corp_tag = tag.select_one(\"div.area_corp .corp_name a\")\n",
    "        corp_name = corp_tag.get_text(strip=True) if corp_tag else \"N/A\"\n",
    "\n",
    "        job_tag = tag.select_one(\"div.area_job .job_tit\")\n",
    "        job_title = job_tag.get_text(strip=True) if job_tag else \"N/A\"\n",
    "\n",
    "        job_key = (corp_name, job_title)\n",
    "        if job_key in collected_jobs:\n",
    "            continue\n",
    "        collected_jobs.add(job_key)\n",
    "\n",
    "        how_apply = tag.select_one(\".sri_btn_xs\")\n",
    "        how_apply = how_apply.get_text(strip=True) if how_apply else \"N/A\"\n",
    "\n",
    "        job_sector_list = tag.select(\".job_sector a\")\n",
    "        job_sector = \", \".join([a.get_text(strip=True) for a in job_sector_list]) if job_sector_list else \"N/A\"\n",
    "\n",
    "        job_condition = tag.select(\".job_condition span\")\n",
    "        loc = job_condition[0].get_text(strip=True) if len(job_condition) > 0 else \"N/A\"\n",
    "        p_h = job_condition[1].get_text(strip=True) if len(job_condition) > 1 else \"N/A\"\n",
    "        degree = job_condition[2].get_text(strip=True) if len(job_condition) > 2 else \"N/A\"\n",
    "        work_division = job_condition[3].get_text(strip=True) if len(job_condition) > 3 else \"N/A\"\n",
    "\n",
    "        data_list.append([corp_name, job_title, how_apply, job_sector, loc, p_h, degree, work_division])\n",
    "\n",
    "    # âœ… AI ì°¨ë‹¨ ë°©ì§€ (5~10ì´ˆ ëœë¤ ëŒ€ê¸°)\n",
    "    sleep_time = random.uniform(5, 10)\n",
    "    print(f\"ğŸ’¤ {sleep_time:.2f}ì´ˆ ëŒ€ê¸° ì¤‘...\")\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "df = pd.DataFrame(data_list, columns=[\"ê¸°ì—…ëª…\", \"ê¸°ì—…ê³µê³ \", \"ì§€ì›ë°©ë²•\", \"ì§ì—… ê´€ë ¨\", \"ì§€ì—­\", \"ê²½ë ¥\", \"í•™ìœ„\", \"ì •ê·œì§ ìœ ë¬´\"])\n",
    "df.to_csv(\"job_info_saramin.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"âœ… í¬ë¡¤ë§ ì™„ë£Œ! {len(df)}ê°œì˜ ê³µê³ ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN1KPq7JdzM/GNxbOrAzPk+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
